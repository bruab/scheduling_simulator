Hardware: HP 2000 Notebook PC with dual core AMD E-450 APU
OS: Ubuntu 14.04 LTS Desktop
Compiler: gcc 4.8.2

***********************************************************

Exercise 1:

1) Only the inner loop can be parallelized without fear of race conditions. Doing the same for the middle or outer loop requires using the 'private' keyword because otherwise, each OpenMP thread has access to the same inner index pointer(s) and can modify them while another thread is accessing them.

2) Here are the average wall clock times for ten runs of each loop parallelization (with N=500):

loop	avg wall time
----	-------------
i	10.7 
k	11.3
j	1.5

Clearly, parallelizing the innermost loop is the best option. This scheme allows the benefits of parallelization without the overhead of critical siections and private variables. I'm not sure why the parallelizing the middle loop should be slower than doing so with the outer loop. I'm guessing they're similar because they have similar restrictions, but the parallel 'i' loop does more work per OpenMP thread, so it's slightly more worthwhile to invest in the overhead.

Also, I reduced N from 1100 (used in Assignment 1) to 500 for this exercise, because the two slower versions of the program were taking forever. The code was already fairly slow in the previous assignment, but the greater N is, the more wait-your-turns there are for critical sections.

3) Ten runs without using OpenMP averaged 3.5 seconds per run with N=1100, while the fastest parallel version took an average of 1.5 seconds. This is a speedup of 2.33. Since I ran the parallel version on two processors, the parallel efficiency is 2.33 / 2 = 1.167 = 116%. This seems impossible, actually, but those are the numbers...

***********************************************************

Exercise 2:

1) This program can't be parallelized with a simple pragma for the i or j loop because for each iteration, each value in the matrix depends on the current values of its neighbors above, below, to the left and to the right. This means that if each thread is given its own 'i', it will require values from the 'i-1' thread and the 'i+1' thread for each step it wishes to take. The same holds true if each thread is given its own 'j'.

2) Wow, this one was hard. I managed to find a couple of ways to iterate over the matrix, one of which was even correct but not parallelizable. My final approach is to first process the element at A[1][1], then to process the elements which depend on its updated value (A[1][2] and A[2][1]), then to process the elements that depend on THOSE updated values, and so on. For N=3, the procedure ends up looking something like this (with '_' representing uncalculated values and 'O' representing calculated values):

XXXXX
X___X
X___X
X___X
XXXXX

XXXXX
XO__X
X___X
X___X
XXXXX

XXXXX
XOO_X
XO__X
X___X
XXXXX

XXXXX
XOOOX
XOO_X
XO__X
XXXXX

XXXXX
XOOOX
XOOOX
XOO_X
XXXXX

XXXXX
XOOOX
XOOOX
XOOOX
XXXXX


3) Here are the stats for running the *sequential* version of the new program (by "original" I mean "exercise2_startingpoint"):

version 	avg. runtime (seconds)
------- 	----------------------
original 	10.24
new 		


Here are the stats for running the program in parallel:

version		num_threads	avg. runtime (seconds)
-------		-----------	----------------------
original	1		10.24
new		1		122.11
new		2		156.88
new		4		170.10

Clearly this version is no improvement, and more threads actually slow things down. I'm assuming that excessive memory accesses and the sheer number of threads required to be created are the reason for the degraded performance. We are all over the matrix at once, so it's impossible to hold all of the values we need in cache for very long, if it's possible to hold them at all. Furthermore, and I suspect more importantly, if the inner for loop is parallelized, that means a different thread executes the calculation of each cell in a diagonal. The longest such diagonal is N=12,000 cells long. Maybe blocking would help. Though I'm skeptical ...

4) 

5) 


Exercise 3:

1) To do a parallel merge, use a recursive, divide-and-conquer approach. Given arrays A and B with target array T, find the median element A[m] of A. Then use binary search to find the first element B[k] of B that is greater than that median element.

We know how many elements in A are less than A[m], and we also know how many elements of B are less than A[m]. So the sum of those two numbers gives us A[m]'s position in the target array T. This leaves us with four arrays which we sort as two pairs: A[1..m-1] and B[1..k-1]; also A[m+1...] and B[k...]. We pass these two pairs of arrays to two new threads, who carry out the same algorithm (including spawning threads of their own). When the arrays are of size 1, they are sorted.

2) Finding the median element of A is O(1); the binary search on B is O(log m). So the overall complexity is O(log m) so far. These steps must be carried out log(n) + 1 times, so the overall complexity is O(log n).
