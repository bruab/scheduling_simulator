Hardware: HP 2000 Notebook PC with dual core AMD E-450 APU
OS: Ubuntu 14.04 LTS Desktop
Compiler: gcc 4.8.2

***********************************************************

Exercise 1:

1) Only the inner loop can be parallelized without fear of race conditions. Doing the same for the middle or outer loop requires using the 'private' keyword because otherwise, each OpenMP thread has access to the same inner index pointer(s) and can modify them while another thread is accessing them.

2) Here are the average wall clock times for ten runs of each loop parallelization (with N=500):

loop	avg wall time
----	-------------
i	10.7 
k	11.3
j	1.5

Clearly, parallelizing the innermost loop is the best option. This scheme allows the benefits of parallelization without the overhead of critical siections and private variables. I'm not sure why the parallelizing the middle loop should be slower than doing so with the outer loop. I'm guessing they're similar because they have similar restrictions, but the parallel 'i' loop does more work per OpenMP thread, so it's slightly more worthwhile to invest in the overhead.

Also, I reduced N from 1100 (used in Assignment 1) to 500 for this exercise, because the two slower versions of the program were taking forever. The code was already fairly slow in the previous assignment, but the greater N is, the more wait-your-turns there are for critical sections.

3) Ten runs without using OpenMP averaged 3.5 seconds per run with N=1100, while the fastest parallel version took an average of 1.5 seconds. This is a speedup of 2.33. Since I ran the parallel version on two processors, the parallel efficiency is 2.33 / 2 = 1.167 = 116%. This seems impossible, actually, but those are the numbers...

***********************************************************

Exercise 2:

1) This program can't be parallelized with a simple pragma for the i or j loop because for each iteration, each value in the matrix depends on the current values of its neighbors above, below, to the left and to the right. This means that if each thread is given its own 'i', it will require values from the 'i-1' thread and the 'i+1' thread for each step it wishes to take. The same holds true if each thread is given its own 'j'.

2) Wow, this one was hard. I managed to find a couple of ways to iterate over the matrix, one of which was even correct but not parallelizable. My final approach is to first process the element at A[1][1], then to process the elements which depend on its updated value (A[1][2] and A[2][1]), then to process the elements that depend on THOSE updated values, and so on. For N=3, the procedure ends up looking something like this (with '_' representing uncalculated values and 'O' representing calculated values):

XXXXX
X___X
X___X
X___X
XXXXX

XXXXX
XO__X
X___X
X___X
XXXXX

XXXXX
XOO_X
XO__X
X___X
XXXXX

XXXXX
XOOOX
XOO_X
XO__X
XXXXX

XXXXX
XOOOX
XOOOX
XOO_X
XXXXX

XXXXX
XOOOX
XOOOX
XOOOX
XXXXX


3) Here are the stats for running the *sequential* version of the new program (by "original" I mean "exercise2_startingpoint"):

version 	avg. runtime (seconds)
------- 	----------------------
original 	10.24
new 		393.77


Here are the stats for running the program in parallel:

version		num_threads	avg. runtime (seconds)
-------		-----------	----------------------
original	1		10.24
new		1		122.11
new		2		156.88
new		4		170.10

Clearly this version is no improvement, and more threads actually slow things down. I'm assuming that excessive memory accesses and the sheer number of threads required to be created are the reason for the degraded performance. We are all over the matrix at once, so it's impossible to hold all of the values we need in cache for very long, if it's possible to hold them at all. Furthermore, and I suspect more importantly, if the inner for loop is parallelized, that means a different thread executes the calculation of each cell in a diagonal. The longest such diagonal is N=12,000 cells long. Maybe blocking would help. Though I'm skeptical ...

4) Here the results of my attempt to find an optimal block size:

num_threads 	block_size 	run time
----------- 	---------- 	--------
1 		12000 		159.92
1 		6000 		195.04
1 		3000 		302.99
1 		1500 		518.99
1 		750 		908.47
1 		375 		1673.79
4 		12000 		154.39
4 		6000 		190.36
4 		3000 		297.28
4 		1500 		510.43
4 		750 		902.19
4 		375 		1666.21

From this I can only guess that I've done something wrong (?). I believe I can still answer the question, though. A block size of N or greater means that each thread is simply updating an entire diagonal of the matrix; the calculation hasn't actually been broken into blocks at all. A block size of 1 means that each thread does a single calculation, and the overhead of managing these threads will be more than it's worth. Somewhere inbetween, there should be a balance where threads have a small enough amount of work to do so that they are memory-efficient, but there aren't so many threads that the OS spends all its time creating and maintaining them.

5) Again, though my numbers are suspect, I'll attempt to answer the question. When executed with 4 threads, the program consistently outperforms the 1-thread version ... by a few seconds. Taking the fastest case as our benchmark, the parallel speedup is 159.92 / 154.39 = 1.036. Which is pretty poor. Parallel efficiency = 1.036 / 2 = 0.518, which is certainly no prize. 

Compared with the original, sequential program, the parallel speedup is less than one. It's a parallel slowdown. Again, I'm not sure if I've done something horribly wrong (most likely) or if this is meant to be a lesson in parallelizaton-resistent algorithms (fingers crossed :). Here are the numbers, anyway: parallel speedup = 10.24 / 154.39 = 0.066. Parallel efficiency = 0.0066 / 2 = 0.0033. Atrocious.


Exercise 3:

1) To do a parallel merge, use a recursive, divide-and-conquer approach. Given arrays A and B with target array T, find the median element A[m] of A. Then use binary search to find the first element B[k] of B that is greater than that median element.

We know how many elements in A are less than A[m], and we also know how many elements of B are less than A[m]. So the sum of those two numbers gives us A[m]'s position in the target array T. This leaves us with four arrays which we sort as two pairs: A[1..m-1] and B[1..k-1]; also A[m+1...] and B[k...]. We pass these two pairs of arrays to two new threads, who carry out the same algorithm (including spawning threads of their own). When the arrays are of size 1, they are sorted.

2) Finding the median element of A is O(1); the binary search on B is O(log m). So the overall complexity is O(log m) so far. These steps must be carried out log(n) + 1 times, so the overall complexity is O(log n).
